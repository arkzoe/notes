# requests
- 请求头中最常见的一些重要内容（爬虫需要）：
	1. User-Agent：请求载体的身份标识（用啥发送的请求）
	2. Referer：防盗链（这次请求是从哪个页面来的？ 反爬会用到）
	3. cookie：本地字符串数据信息（用户登录信息，反爬的token）
- 响应头中一些重要的内容：
	1. cookie：本地字符串数据信息（用户登录信息，反爬的token）
	2. 各种神奇的莫名其妙的字符串（这个需要经验了，一般都是token字样，防止各种攻击和反爬）
```python
import requests
# get请求  
content = input('检索内容：')  
url = f"https://cn.bing.com/search?q={content}"  
headers = {  
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"  
}  
response = requests.get(url, headers=headers)  
print(response.text)  
print(response.request.headers)  
  
# post请求  
url = "https://fanyi.baidu.com/sug"  
data ={  
    "kw":input("请输入单词")  
}  
response = requests.post(url,data=data)  
print(response.text) 字符串  
print(response.json()['data'])json  
  
# 多参数get  
url = "https://movie.douban.com/j/chart/top_list"  
data ={  
    "type":"13",  
    "interval_id":"100:90",  
    "action":"",  
    "start":"0",  
    "limit":"20"  
}  
headers = {  
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0"  
}  
r = requests.get(url,params=data,headers=headers)  
print(r.json())  
print(r.request.url)
```
1. 模拟浏览器登录->处理cookie
```python
# 登录->得到cookie
# 带着cookie 去请求到书架url ->书架上的内容
# 必须得把上面的两个操作连起来
# 我们可以使用session进行请求 -> session你可以认为是一连串的请求。在这个过程中的cookie不会丢失
import requests
# 会话
session = requests.session()
session.post(url)

# 从浏览器复制cookie
headers = {  
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0",  
    "cookie":"Hm_lvt_ef8d5b3eafdfe7d1bbf72e3f450ad2ed=1764912096; cf_clearance=Z6_C2VzAgC8xgbSl.CgGs4L5.ubJcYAezYSFtaYQzdE-1765876464-1.2.1.1-GgsvR5YYg1rLSnnqLgbyuH32VvN1NeNSNp4AXY.acNwBpVxUFDrhQiKdzn6gYnekf3QtVnDlihPJd2ofzr8immN.g3UzyJRL88tmtw6vsLCT4Y_iDC6oCidDw5gvJtCmJxd.i5KYQjUMWX0Bn5lM2zgBBMQKpVJ.UpsDQi4ZxtOg0pfaI0X2cOaKONp6B0vg6gHBh1oM2KQi3JKX2Yon.bucMpRA5NdZx8KhSGovNcDCfjVO_95kYgAzBz4smoN7;PHPSESSID=p2v99e126j2dhrmmkntenputb8;jieqiUserInfo=jieqiUserId%3D707614%2CjieqiUserUname%3D%E6%B2%90%E6%99%B4%2CjieqiUserName%3D%E6%B2%90%E6%99%B4%2CjieqiUserGroup%3D3%2CjieqiUserGroupName%3D%E6%99%AE%E9%80%9A%E4%BC%9A%E5%91%98%2CjieqiUserVip%3D0%2CjieqiUserHonorId%3D3%2CjieqiUserHonor%3D%E4%B8%AD%E4%BA%8C%2CjieqiUserToken%3D9fc3cfc3ada76a8ef3fb19238a3c0b09%2CjieqiCodeLogin%3D0%2CjieqiCodePost%3D0%2CjieqiUserLogin%3D1765876873; jieqiVisitInfo=jieqiUserLogin%3D1765876873%2CjieqiUserId%3D707614"  
} 
url = "https://www.linovelib.com/modules/article/bookcase.php"  
response = requests.get(url,headers=headers)
```
2. 防盗链处理->抓取梨视频数据
```python
headers = {
	# 防盗链：溯源，当前请求上一级是谁
	"Referer":"请求上级--url"
}
#拉取视频的网址
url = "https://www.pearvideo.com/video_1721605"
contId = url.split("_")[1]
videoStatusUrl = f"https://www.pearvideo.com/videoStatus.jsp?contId={contId}"
headers ={
	"User-Agent":""
	"Referer":"url"
}
resp = requests.get(videoStatusUrl, headers=headers)
dic = resp.json()
srcUrl = dic['videoInfo']['videos'Jl'srcUrl']
systemTime = dic['systemTime']
srcUrl = srcUrl.repalce(systemTime, f"cont-{contId}")
with open("a.mp4",mod="wb") as f:
	f.write(requests.get(srcUrl).content)
```
3. 代理->防止被封ip
- 使用第三方机器代理请求
- 缺点：慢，ip不好找
```python
# 准备代理信息
proxy = {
	"http":"http://ip:port",
	"https":"https://ip:port"
}
resp = requests.get(url,proxies=proxy)
```

| 特性           | 透明代理                                                   | 匿名代理                                                     | 高匿代理                                                     |
| ------------ | ------------------------------------------------------ | -------------------------------------------------------- | -------------------------------------------------------- |
| 目标网站看到的IP    | 你的真实 IP                                                | 代理服务器的IP                                                 | 代理服务器的IP                                                 |
| 是否暴露使用了代理    | 是 (通过`VIA`头等)                                          | 是 (通过`VIA`头等)                                            | 否                                                        |
| 是否隐藏真实IP     | 否                                                      | 是                                                        | 是                                                        |
| 主要目的         | 缓存内容、<br>内容过滤、<br>企业监控                                 | 隐藏真实IP，<br>绕过简单IP限制                                      | 最高程度的隐私保护，<br>绕过绝大多数IP封锁和检测                              |
| HTTP头信息      | 会添加`HTTP_VIA`，<br>`HTTP_X_FORWARDED_FOR`<br>(包含你的真实IP) | 会添加 `HTTP_VIA` ,<br>`HTTP_X_FORWARDED_FOR`<br>为空、乱码或代理IP | 不发送或模拟发送<br>与普通客户端无异的头信息<br>不暴露`VIA` , `X-FORWARDED-FOR` |
| 被目标服务器识别的难易度 | 极易识别                                                   | 较易识别                                                     | 难以识别，看起来像普通用户                                            |

4. 接入第三方代理
```python
# 第三方生成的链接，请求到最大值再拿一次
def get_ip():
	url = ""
	reap = requests.get(url)
	ips = resp.json()
	for ip in ips['data'j['proxy_list']:
		yield ip
gen = get_ip()
```
# 数据解析
## re解析
- 速度快、效率高、准确性高
- 元字符：具有固定含义的特殊字符
```python
.       //匹配除换行外任意字符
\w      //匹配任意数字、字母、下划线
\s      //匹配任意空白符
\d      //匹配数字
\n      //匹配一个换行符
\t      //匹配一个制表符

^       //匹配字符串开始
$       //匹配字符串结尾

\W      //匹配任意非数字、字母、下划线
\S      //匹配任意非空白符
\D      //匹配非数字
a|b     //匹配字符a或字符b
()      //匹配括号内的表达式，也表示一个组
[...]   //匹配字符组中的字符
[^...]  //匹配除字符组中字符的所有字符
```
- 量词：控制元字符出现次数
```python
*     //重复零次或多次
+     //重复一次或多次
?     //重复零次或一次
{n}   //重复n次
{n,}  //重复n次或多次
{n,m} //重复n到m次
```
- 贪性匹配和惰性匹配
```python
.*    //贪性，尽可能多
.*?   //惰性，尽可能少，回溯
```
- re模块
```python
import re
request = re.findall(r"a.*?a","aaaa13231aaa")  
print(request)  
  
res = re.finditer(r"\d+","100,114514")  
for match in res:       #从迭代器中拿到内容  
    print(match.group())#从匹配中拿到数据  

# search只匹配第一次匹配到的  
r = re.search(r"\d+","100,114514")  
print(r.group())  
  
# match，匹配时从开头匹配，相当与加^  
r = re.match(r"\d+","100,114514")  
print(r.group())

# 预加载  compile
obj = re.compile(r"\d+")
```
- (?P< 组 >表达式)
## bs4解析
```python
# 1.初始化BeautifulSoup对象
page = Beautifulsoup(html, "html.parser")
page.find("标签名"，attrs={"属性”：“值"})    #查找某个元素，只会找到一个结果
page.findall("标签名"，attrs={"属性":"值"})  #找到一堆结果

li = page.find("li", attrs={"id":"abc"})
a = li.find("a")
print(a.text)        #拿文本
print(a.get("href")) #拿属性.get（"属性名"）

li_list = page.find_all("li")
for li in li_list:
	a = li.find("a")
	text = a.text
	href = a.get("href")
	print(text,href)
```
## xpath解析
- XPath是一门在XML文档中查找信息的语言。XPath可用来在XML文档中对元素和属性进行遍历。而我们熟知的HTML恰巧属于XML的一个子集，所以完全可以用xpath去查找html中的内容
```python
from lxml import etree

et = etree.XML(html)
et.xpath("/book") # /表示根节点
result =et.xpath("/book/name"）         # 在xpath中间的/表示的是儿子
result =et.xpath("/book/name/text())")  # text（） 拿文本
result =et.xpath("/book//nick"）        # //表示的是子孙后代
result =et.xpath("/book/*/nick"）       # * 通配符
result =et.xpath("/book/author/nick[@class='jay']/text（)"）# 门表示属性筛选。@属性名=值
result =et.xpath("/book/partner/nick/@id"）#最后一个/表示拿到nick里面的id的内容，@属性。可以直接拿到属性值

li_list = et.xpath("/html/body/ul/li[2]/a/text()") # 拿第2个li
li_list = et.xpath("//li")
for li in li_list:
	href =li.xpath("./a/ahref"）[0]  #./表示当前节点
	text =li.xpath("./a/text（)")[0] #./表示当前节点
	#后续的爬虫工作····
```
## pyquery解析
- 通过css选择器筛选
```python
html="""
	<ul>
		<li class="aaa"><a href="http: //www.google.com">谷歌</a></li>
		<li class="aaa"><a href="http://www.baidu.com">百度</a></li>
		<Li class="bbb" id="qq">xa href="http://www.qq.com">腾讯</a>
		</Li><li class="bbb"><a href="http://www.yuanlai.com">猿来</a></Li>
	</uL>
"""

#加载html内容
p = PyQuery(html)
# print(p)
# print(type(p))  pyauery对象
# pyauery对象直接（css选择器）
li = p("li")   # pyauery对象
a = p("li a")  # 后代选择/("a")

href = p("#qq a").attr("href") # http://www.qq.com
href = p("#qq a").text()       # 腾讯

#多个标签拿属性
it = p("li a").items() # 迭代器
from item in it:
	href = item.attr("href")
```
1. pyguery(选择器）
2. items()：当选择器选择的内容很多的时候。需要一个一个处理的时候
3. attr(属性名)：获取属性信息
4. text()：获取文本，过滤html
5. html()：全都要
```python
from pyquery import PyQuery as pq
html = """
	<HTML>
		<div class="aaa">哒哒哒</div>
		<div class="bbb">嘟嘟嘟</div>
	</HTML>
"""
doc = pq(html)
doc(".aaa").after("""<div class="ccc">妈呀</div>"""）# 插入HTML代码片段,后面添加
doc(".bbb").append("<Span>我爱span</span>"）         # 向HTML内层标签中插入HTML片段
doc(".aaa").html("<span>我是span</span>"）           # 修改标签内的html代码
doc(".ccc").text("美滋滋"）                          # 修改文本内容
doc(".ccc").attr("cs"，"测试")                       # 添加属性
doc(".ccc").remove_attr("cs")                       # 删除属性
doc(".ccc").remove()                                # 删除标签
print(doc)
```
# 多线程
- 进程：资源单位
- 线程：执行单位
```python
from threading import Thread

t = Thread(target=,args={"",}) # 创建线程安排任务
t.start() # 可以开始工作状态

class MyThread(Thread):
	def run(self):
		pass
t = MyThread()
t.start()
```
## 多进程
```python
from multiprocessing import Process

t = Process(target=) # 创建线程安排任务
t.start() # 可以开始工作状态

```